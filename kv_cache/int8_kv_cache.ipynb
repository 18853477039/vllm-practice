{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T07:05:21.998637Z",
     "start_time": "2025-03-14T07:05:18.728868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "4c626640f9069877",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jishoukai/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T07:05:22.081996Z",
     "start_time": "2025-03-14T07:05:22.080285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# constants\n",
    "max_tokens = 50\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=max_tokens)"
   ],
   "id": "817e130461934442",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## bp16",
   "id": "42304d0fdcb8b250"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T07:05:22.510530Z",
     "start_time": "2025-03-14T07:05:22.092116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B\"\n",
    "\n",
    "llm = LLM(model=model, gpu_memory_utilization=0.9)\n"
   ],
   "id": "187b121797ccbba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-14 15:05:22 __init__.py:207] Automatically detected platform cpu.\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHFValidationError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m model = \u001B[33m\"\u001B[39m\u001B[33m/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m llm = \u001B[43mLLM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgpu_memory_utilization\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/utils.py:1022\u001B[39m, in \u001B[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1015\u001B[39m             msg += \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00madditional_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1017\u001B[39m         warnings.warn(\n\u001B[32m   1018\u001B[39m             \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m(msg),\n\u001B[32m   1019\u001B[39m             stacklevel=\u001B[32m3\u001B[39m,  \u001B[38;5;66;03m# The inner function takes up one level\u001B[39;00m\n\u001B[32m   1020\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1022\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/entrypoints/llm.py:242\u001B[39m, in \u001B[36mLLM.__init__\u001B[39m\u001B[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001B[39m\n\u001B[32m    239\u001B[39m \u001B[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[38;5;66;03m# to avoid import order issues\u001B[39;00m\n\u001B[32m    241\u001B[39m \u001B[38;5;28mself\u001B[39m.engine_class = \u001B[38;5;28mself\u001B[39m.get_engine_class()\n\u001B[32m--> \u001B[39m\u001B[32m242\u001B[39m \u001B[38;5;28mself\u001B[39m.llm_engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_engine_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    243\u001B[39m \u001B[43m    \u001B[49m\u001B[43mengine_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musage_context\u001B[49m\u001B[43m=\u001B[49m\u001B[43mUsageContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mLLM_CLASS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[38;5;28mself\u001B[39m.request_counter = Counter()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/engine/llm_engine.py:486\u001B[39m, in \u001B[36mLLMEngine.from_engine_args\u001B[39m\u001B[34m(cls, engine_args, usage_context, stat_loggers)\u001B[39m\n\u001B[32m    484\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001B[39;00m\n\u001B[32m    485\u001B[39m \u001B[38;5;66;03m# Create the engine configs.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m486\u001B[39m engine_config = \u001B[43mengine_args\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_engine_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43musage_context\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    487\u001B[39m executor_class = \u001B[38;5;28mcls\u001B[39m._get_executor_cls(engine_config)\n\u001B[32m    488\u001B[39m \u001B[38;5;66;03m# Create the LLM engine.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1127\u001B[39m, in \u001B[36mEngineArgs.create_engine_config\u001B[39m\u001B[34m(self, usage_context)\u001B[39m\n\u001B[32m   1122\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cpu_offload_gb >= \u001B[32m0\u001B[39m, (\n\u001B[32m   1123\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mCPU offload space must be non-negative\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1124\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.cpu_offload_gb\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1126\u001B[39m device_config = DeviceConfig(device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1127\u001B[39m model_config = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcreate_model_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (model_config.is_multimodal_model \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m envs.VLLM_USE_V1\n\u001B[32m   1130\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.enable_prefix_caching):\n\u001B[32m   1131\u001B[39m     logger.warning(\u001B[33m\"\u001B[39m\u001B[33m--enable-prefix-caching is currently not \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1132\u001B[39m                    \u001B[33m\"\u001B[39m\u001B[33msupported for multimodal models in v0 and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1133\u001B[39m                    \u001B[33m\"\u001B[39m\u001B[33mhas been disabled.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1047\u001B[39m, in \u001B[36mEngineArgs.create_model_config\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1046\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_model_config\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> ModelConfig:\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mModelConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1049\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# We know this is not None because we set it in __post_init__\u001B[39;49;00m\n\u001B[32m   1051\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m        \u001B[49m\u001B[43mallowed_local_media_path\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mallowed_local_media_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1057\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrope_scaling\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrope_scaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrope_theta\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrope_theta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1061\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_overrides\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhf_overrides\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1062\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_revision\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_model_len\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_model_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1064\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquantization\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mquantization\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m        \u001B[49m\u001B[43menforce_eager\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menforce_eager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_seq_len_to_capture\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_seq_len_to_capture\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_logprobs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1068\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisable_sliding_window\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdisable_sliding_window\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1069\u001B[39m \u001B[43m        \u001B[49m\u001B[43mskip_tokenizer_init\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mskip_tokenizer_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1070\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserved_model_name\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mserved_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1071\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlimit_mm_per_prompt\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlimit_mm_per_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1072\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_async_output_proc\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdisable_async_output_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1073\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig_format\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1074\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmm_processor_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmm_processor_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1075\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisable_mm_preprocessor_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdisable_mm_preprocessor_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[43m        \u001B[49m\u001B[43moverride_neuron_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moverride_neuron_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1077\u001B[39m \u001B[43m        \u001B[49m\u001B[43moverride_pooler_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moverride_pooler_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1078\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor_pattern\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlogits_processor_pattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1079\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1080\u001B[39m \u001B[43m        \u001B[49m\u001B[43moverride_generation_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moverride_generation_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[43m        \u001B[49m\u001B[43menable_sleep_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menable_sleep_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1082\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_impl\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_impl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1083\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/config.py:304\u001B[39m, in \u001B[36mModelConfig.__init__\u001B[39m\u001B[34m(self, model, task, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, allowed_local_media_path, revision, code_revision, rope_scaling, rope_theta, tokenizer_revision, max_model_len, spec_target_max_model_len, quantization, enforce_eager, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name, limit_mm_per_prompt, use_async_output_proc, config_format, hf_overrides, mm_processor_kwargs, disable_mm_preprocessor_cache, override_neuron_config, override_pooler_config, logits_processor_pattern, generation_config, enable_sleep_mode, override_generation_config, model_impl)\u001B[39m\n\u001B[32m    301\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.enable_sleep_mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m current_platform.is_cuda():\n\u001B[32m    302\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mSleep mode is only supported on CUDA devices.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m hf_config = \u001B[43mget_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m                       \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hf_overrides_kw:\n\u001B[32m    308\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mOverriding HF config with \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, hf_overrides_kw)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/transformers_utils/config.py:256\u001B[39m, in \u001B[36mget_config\u001B[39m\u001B[34m(model, trust_remote_code, revision, code_revision, config_format, **kwargs)\u001B[39m\n\u001B[32m    253\u001B[39m     model = Path(model).parent\n\u001B[32m    255\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config_format == ConfigFormat.AUTO:\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_gguf \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mfile_or_path_exists\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mHF_CONFIG_NAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    258\u001B[39m         config_format = ConfigFormat.HF\n\u001B[32m    259\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m file_or_path_exists(model, MISTRAL_CONFIG_NAME,\n\u001B[32m    260\u001B[39m                              revision=revision):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/vllm/transformers_utils/config.py:168\u001B[39m, in \u001B[36mfile_or_path_exists\u001B[39m\u001B[34m(model, config_name, revision)\u001B[39m\n\u001B[32m    165\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (local_path / config_name).is_file()\n\u001B[32m    167\u001B[39m \u001B[38;5;66;03m# Offline mode support: Check if config file is cached already\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m168\u001B[39m cached_filepath = \u001B[43mtry_to_load_from_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m                                         \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m                                         \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cached_filepath, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    172\u001B[39m     \u001B[38;5;66;03m# The config file exists in cache- we can continue trying to load\u001B[39;00m\n\u001B[32m    173\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m arg_name, arg_value \u001B[38;5;129;01min\u001B[39;00m chain(\n\u001B[32m    102\u001B[39m     \u001B[38;5;28mzip\u001B[39m(signature.parameters, args),  \u001B[38;5;66;03m# Args values\u001B[39;00m\n\u001B[32m    103\u001B[39m     kwargs.items(),  \u001B[38;5;66;03m# Kwargs values\u001B[39;00m\n\u001B[32m    104\u001B[39m ):\n\u001B[32m    105\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mrepo_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfrom_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mto_id\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m         \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    108\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m arg_name == \u001B[33m\"\u001B[39m\u001B[33mtoken\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    109\u001B[39m         has_token = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/vllm_practice/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001B[39m, in \u001B[36mvalidate_repo_id\u001B[39m\u001B[34m(repo_id)\u001B[39m\n\u001B[32m    151\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRepo id must be a string, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(repo_id)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m repo_id.count(\u001B[33m\"\u001B[39m\u001B[33m/\u001B[39m\u001B[33m\"\u001B[39m) > \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[32m    155\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRepo id must be in the form \u001B[39m\u001B[33m'\u001B[39m\u001B[33mrepo_name\u001B[39m\u001B[33m'\u001B[39m\u001B[33m or \u001B[39m\u001B[33m'\u001B[39m\u001B[33mnamespace/repo_name\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    156\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m. Use `repo_type` argument if needed.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    157\u001B[39m     )\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX.match(repo_id):\n\u001B[32m    160\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[32m    161\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRepo id must use alphanumeric chars or \u001B[39m\u001B[33m'\u001B[39m\u001B[33m-\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m_\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m--\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33m..\u001B[39m\u001B[33m'\u001B[39m\u001B[33m are\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    162\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m forbidden, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m-\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m\u001B[33m cannot start or end the name, max length is 96:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    163\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    164\u001B[39m     )\n",
      "\u001B[31mHFValidationError\u001B[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []\n",
    "\n",
    "batch_sizes = [2 ** p for p in range(10)]\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs={batch_size}\")\n",
    "\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompt = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    outputs = llm.generate(batch_prompt, sampling_params)\n",
    "\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    # calculate throughput\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(f\"duration: {duration_s}\")\n",
    "    print(f\"throughput: {throughput} tokens/s\")\n",
    "    print(f\"avg latency: {avg_latency}\")\n",
    "    print()\n",
    "\n",
    "    durations.append(duration_s)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)\n"
   ],
   "id": "95f9e034856755c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save observations and restart kernel\n",
    "# save as json\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"durations\": durations,\n",
    "    \"throughputs\": throughputs,\n",
    "    \"latencies\": latencies,\n",
    "    \"batch_sizes\": batch_sizes,\n",
    "    \"prompts\": prompts,\n",
    "    \"max_tokens\": max_tokens\n",
    "\n",
    "}\n",
    "\n",
    "with open(\"basic_fp16_inference_observation.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n"
   ],
   "id": "5feb8a0a4880f007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# Read the data from the JSON file\n",
    "with open(\"basic_fp16_inference_observation.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Restore the lists\n",
    "durations = data[\"durations\"]\n",
    "throughputs = data[\"throughputs\"]\n",
    "latencies = data[\"latencies\"]\n",
    "batch_sizes = data[\"batch_sizes\"]\n",
    "prompts = data[\"prompts\"]\n",
    "max_tokens = data[\"max_tokens\"]"
   ],
   "id": "c3264323c833fe42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## int8 kv cache",
   "id": "c382fbf953e974bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "id": "79e2a5ba7f269d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=max_tokens)",
   "id": "218db9b8d0638b4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B\"\n",
    "\n",
    "llm_with_kv_cache = LLM(model=model,\n",
    "                        kv_cache_dtype=\"auto\",\n",
    "                        calculate_kv_scales=True,\n",
    "                        gpu_memory_utilization=0.9)"
   ],
   "id": "a919507be45bbf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# observations\n",
    "durations_with_kv_cache = []\n",
    "throughputs_with_kv_cache = []\n",
    "latencies_with_kv_cache = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs={batch_size}\")\n",
    "\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompt = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    outputs = llm_with_kv_cache.generate(batch_prompt, sampling_params)\n",
    "\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    # calculate throughput\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(f\"duration: {duration_s}\")\n",
    "    print(f\"throughput: {throughput} tokens/s\")\n",
    "    print(f\"avg latency: {avg_latency}\")\n",
    "    print()\n",
    "\n",
    "    durations_with_kv_cache.append(duration_s)\n",
    "    throughputs_with_kv_cache.append(throughput)\n",
    "    latencies_with_kv_cache.append(avg_latency)"
   ],
   "id": "b976d42cdf808597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def render_plot(x, y1, y2, y3, y4, x_label, y1_label, y2_label):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # plot the first line (FP32 throughput)\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=color)\n",
    "    ax1.plot(x, y1, color=color, label='FP16 Throughput', linestyle='-')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # plot the third line (kv cache throughput)\n",
    "    ax1.plot(x, y3, color=color, label='kv cache Throughput', linestyle='--')\n",
    "\n",
    "    # set the x-axis to be log scaled\n",
    "    ax1.set_xscale('log', base=2)\n",
    "\n",
    "    # Instantiate a second axes shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2_label, color=color)\n",
    "    ax2.plot(x, y2, color=color, label='FP16 Latency', linestyle='-')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # plot the fourth line (kv cache latency)\n",
    "    ax2.plot(x, y4, color=color, label='kv cache Latency', linestyle='--')\n",
    "\n",
    "    # Add legends\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c92b948af0f29eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "render_plot(\n",
    "    batch_sizes,\n",
    "    throughputs,\n",
    "    latencies,\n",
    "    throughputs_with_kv_cache,\n",
    "    latencies_with_kv_cache,\n",
    "    \"batch size\",\n",
    "    \"throughput (tokens/s)\",\n",
    "    \"avg latency (s)\"\n",
    ")"
   ],
   "id": "f075bb20dbbaa840"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
